---
title: "Interactive data graphics (with Shiny) and Text Data"
author: "Vicki Hertzberg"
date: "February 12, 2020"
output:
  html_document: default
---

Today we are going to go through a few topics. We are going to start with interactive data graphics, and we will talk about making Shiny apps in R. We will also talk about dealing with text data. 

## Interactive Data Graphics

With Web 2.0, web browsers became more complex in the mid-00's along with an increased demand for interactive data visualizations in a browser. To date, all we have discussed are static images, but there are tools that make it easy (or easier) to make interactive data graphics.

One of these is JavaScript. With JavaScript, the computations are taking place on the _client side_, not on the host's web server. 

The current state of the art for client-side dynamic data graphics on the web is D3, a JavaScript Library. D3 stands for "data-driven documents."

How to do with with R then? The developers of R Studio have come to the rescue with the \textcolor{red}{htmlwidgets} package, which allows R developers to create packages that render graphics in HTML using D3. In other words, R developers can make use of D3 without having to learn D3. Moreover, since this is happening on the R Studio side, R users can embed these graphics in annotated web documents.

One such \textcolor{red}{htmlwidgets} tool is Plot.ly, which is really a project to develop the ability to generate data graphics between R, Python, and other tools. It is based on the `plotly.js` JavaScript library. In R we get the functionality of Plot.ly by using the `plotly` package.

An especially attractive feature of `plotly` is that it can convert any `ggplot2` object into a `plotly` object using the `ggplotly()` function. It supports the following capabilities:

- _brushing_: marking selected points
- _mouse-over_: points display additional information when the mouse hovers over them
- _panning_: moving across the viewing pane in a parallel direction
- _zooming_: moving into an image.

Let's do an example. The package `babynames` contains a dataset listing the names of all babies born in the US since 1880. We will use this dataset to determine the frequency with which babies were given the names of one of the members of the Beatles over time.

```{r}
# Create dataframe by grabbing the data then filtering
library(ggplot2)
library(plotly)
library(babynames)
Beatles <- babynames %>%
  filter(name %in% c("John", "Paul", "George", "Ringo") & sex == "M")

# Build the plot

beatles_plot <- ggplot(data = Beatles, aes(x=year, y = n)) + 
  geom_line(aes(color=name), size=2)
beatles_plot

# Make it interactive

ggplotly(beatles_plot)

```
So use the tools for mousing, brushing, etc.

Another of the htmlwidgets is the DT (i.e., DataTables) package that makes data tables interactive. Let's look at an example with our Beatles names data.

```{r}
# Build a dynamic table

library(DT)
datatable(Beatles, options = list(pageLength = 25))
```

Pretty cool, huh!

Another tool for dynamic visualization is the `\textcolor{red}{ggvis}` package. This package is _not_ built using the D3 or \textcolor{red}{htmlwidgets} frameworks. Let's use it to create a visualization of the proportion of male babies named John as a function of the number of names over time, such that the user can mouse over a values to see the year, number, and proportion.

```{r}
library(ggvis)
# Find out how many males named John
John <- filter(Beatles, name=="John")

# Find out how many males
all_values <- function(x){
  if (is.null(x)) return(NULL)
  row <- John[John$year == x$year, ]
  paste0(names(row), ": ", format(row), collapse = "<br />")
}

John %>%
  ggvis(~n, ~prop, fill = ~year) %>%
  layer_points() %>%
  add_tooltip(all_values, "hover")
```

## And then there is _Shiny_

Shiny is an R package that can turn your analyses into interactive web applications. It is beyond the scope of the course, not because it is at an advanced level, but more that it is a topic for which we need more time to teach, and we don't have that time in the syllabus. We are not going to tell you _how_ to make a shiny application here, but we are going to show you a couple of examples and tell you how to learn more.

### Example 1

See (https://cpelat.shinyapps.io/mass/)[https://cpelat.shinyapps.io/mass/] for disease surveillance in France.

### Example 2

See (https://miningthedetails.shinyapps.io/knn-dashboard-shiny-plotly/)[https://miningthedetails.shinyapps.io/knn-dashboard-shiny-plotly/] for data mining to predict heart disease.

The reason that we bring up Shiny is that with another 3 hours or  so, you could build your own Shiny app. You have the skills now in terms of R coding, it would just take Melinda and me a couple of hours to get you through all of the steps necessary to do so. If you are interested, there are a variety of online tutorials, see 

- (http://shiny.rstudio.com/tutorial/)[http://shiny.rstudio.com/tutorial/]
- (http://deanattali.com/blog/building-shiny-apps-tutorial/)[http://deanattali.com/blog/building-shiny-apps-tutorial/]


## Working with Text

Machines ==> Good at storing text, not so good at understanding text

Humans ==> Good at understanding text, not so good at storing text

To process text, you need an extra set of wrangling skills. We are now going to introduce how to 
- _ingest_ text
- create _corpora_ (collections of text document)
- use _regular expressions_ to automate text searches 

We are going to use text mining techniques to explore the play _The Tragedy of Hamlet, Prince of Denmark_, aka _Hamlet_. To do this, we are going to pull the text from the Project Gutenberg site, (http://www.gutenberg.org)[http://www.gutenberg.org]. Project Gutenberg has over 53,000 electronic books and other text documents that you can download _for free_.

```{r}

# Get the text of Hamlet

library(RCurl)
hamlet_url <- "http://www.gutenberg.org/cache/epub/1787/pg1787.txt"
Hamlet_raw <- RCurl::getURL(hamlet_url)

```

Note that the object `Hamlet_raw` is a *single string* of text containing the entire play. 

To work with this, we are going to have to split this string into a vector of strings, and we will do this using the function `strsplit()`. We will also have to specify the end of line character(s), which in this case are: `\r\n`.

```{r}

# split string at end of line codes (\r\n); this returns a list - we only want the first element in the list
hamlet <- strsplit(Hamlet_raw, "\r\n")[[1]]
length(hamlet)

```
Now let's examine the text:

```{r}
#examine the text - list out some elements
hamlet[350:360]
```

Notice that in this new `hamlet` object there are two spaces between the start of a line, then an abbreviation for the speaker's name, then another space. We can take advantage of this and other patterns to quantify the ideas within the text. As an example, let's see how many times Hamlet speaks.

```{r}

# get the number of lines that Hamlet speaks
hamlet_lines <- grep("  Ham. ", hamlet, value = TRUE)
length(hamlet_lines)

# inspect the data
head(hamlet_lines)
```

Look at the difference if we don't consider the space after the abbreviation in our expression to evaluate:

```{r}

# get the number of lines that Hamlet speaks
hamlet_lines <- grep("  Ham.", hamlet, value = TRUE)
length(hamlet_lines)

# inspect the data
head(hamlet_lines)
```

We will see that happens down below. 

The `grep()` function is what you use when you want to use R to find a needle in a haystack. The first argument to the function is a _regular expression_ (i.e., a pattern) that you want to find, and the second argument is the character vector in which you want to find the patterns. Note that if you do not include "value = TRUE", the function will return the indices of the haystack in which the needles were found. To illustrate, let's look for Ophelia's lines;

```{r}

# find Ophelia's lines
ophelia_lines <- grep(" Oph. ", hamlet)
length(ophelia_lines)
head(ophelia_lines)
```

The function `grepl()` uses the same syntax but returns instead a logical vector as long as the haystack. For example,

```{r}

#illustrate differences between grep and grepl
length(grep("  Ham. ", hamlet))
length(grepl("  Ham. ", hamlet))
```

To extract the piece of each matching line that actually matched, use the `str_extract()` function from the `stringr` package.

```{r}

# Extract the lines that match
library(stringr)
pattern <- "  Ham. "
grep(pattern, hamlet, value = TRUE) %>%
  str_extract(pattern) %>%
  head()

```

*Regular expressions* are very powerful and very commonly used in many different environments. Understanding the concept of regular expressions will pay off in the long term.

# Regular Expression Syntax

- *.* is a metacharacter that matches any character. If you want to search for the literal values of a metacharacter, you have to use two backslashes in R.

```{r}
# Example of use of . as a metacharacter

hamlet_lines <- grep("  Ham.", hamlet, value = TRUE)
length(hamlet_lines)

hamlet_lines <- grep("  Ham. ", hamlet, value = TRUE)
length(hamlet_lines)

hamlet_lines <- grep("  Ham\\.", hamlet, value = TRUE)
length(hamlet_lines)

```

- *Character sets:* Use brackets to define sets of characters to match.

```{r}

# Example os use of character sets

head(grep("H[b-z]", hamlet, value = TRUE))
```

This notation will result in each occurrence of H followed by any small letter except "a".

- *Alternation:* For this we use the symbol `|` within parentheses.

```{r}

# Example of alternation

head(grep("  H(a|o)", hamlet, value = TRUE))

```

So you see that using the `  H(a|o)` alternation allows us to pick up any lines in which occurs the sequence "  Ha" or "  Ho".

- *Anchors:* Use `^` to anchor a pattern to the beginning of a text, and use `$` to anchor it to the end.

```{r}

# Example of anchor at the beginning
head(grep("^  H[b-z]", hamlet, value = TRUE))

```

- *Repetitions:* Specify the number of times that we want a certain pattern to occur.

    - `?` means zero or one time
    - `*` means zero or more times
    - `+` means one or more times
    
```{r}

# Examples of repetitive patterns

head(grep("  $H(a|o)", hamlet, value = TRUE))

```

```{r}

# Examples of repetitive patterns

head(grep("  *H(a|o)", hamlet, value = TRUE))
```

```{r}

# Examples of repetitive patterns

head(grep("  +H(a|o)", hamlet, value = TRUE))
```

So how do we use these techniques to analyze the text?

- What can we learn by noting who speaks when?
_ When does each character speak as a function of the line number in the play?


```{r}

# Assign the characters
Hamlet <- grepl("  Ham\\.", hamlet)
Ophelia <- grepl("  Oph\\.", hamlet)
Polonius <- grepl("  Pol\\.", hamlet)
Gertrude <- grepl("  Queen\\.", hamlet)
Laertes <- grepl("  Laer\\.", hamlet)
Claudius <- grepl("  King\\.", hamlet)
Horatio <- grepl("  Hor\\.", hamlet)
Fortinbras <- grepl("  For\\.", hamlet)

length(Hamlet)
length(Ophelia)
length(Polonius)
length(Gertrude)
length(Laertes)
length(Claudius)
length(Horatio)
length(Fortinbras)

sum(Hamlet)
sum(Ophelia)
sum(Polonius)
sum(Gertrude)
sum(Laertes)
sum(Claudius)
sum(Horatio)
sum(Fortinbras)


```

Before we can use these data, we want to convert the *logical* vectors into *numeric* vectors, then tidy the data. There is also a bunch of unwanted text at the beginning and end of the raw text, and we want to get rid of it so that our analyses just pertain to the corpus of the play itself, which starts at line 274 and extends to line 5130.

```{r}

# Let's tidy up the dataset
library(tidyverse)

speaker_freq <- data.frame(Hamlet, Polonius, Claudius, Horatio, Ophelia, Gertrude, Laertes) %>%
  mutate(line=1:length(hamlet)) %>%
  gather(key = "character", value = "speak", -line) %>%
  mutate(speak = as.numeric(speak)) %>%
  filter(line > 273 & line < 5131)

glimpse(speaker_freq)
  
```

Another thing to do before we do the deep dive: Let's create some context of helpful information, namely the lines at which each Act starts and stops.

```{r}

# Delineate the Acts
acts_idx <- grep("^A[C|c][T|t] [I|V]+", hamlet)
acts_labels <- str_extract(hamlet[acts_idx], "^A[C|c][T|t] [I|V]+")
acts <- data.frame(line=acts_idx, labels = acts_labels)

```

Now let's see when these 4 characters are speaking throughout the play:

```{r}

# Plot the lines at which each character speaks
ggplot(data = speaker_freq, aes(x=line, y=speak)) +
  geom_smooth(aes(color=character), method = "loess", se = 0, span = 0.4) +
  geom_vline(xintercept = acts_idx, color = "darkgray", lty = 3) +
  geom_text(data = acts, aes(y=0.085, label = labels), hjust = "left", color = "darkgray") +
  ylim(c(0, NA)) +
  xlab("Line Number") +
  ylab("Proportion of Speeches")

```

We can also ingest text by scraping it from the internet. Let's look at the discography of David Bowie, as listed on Wikipedia:


```{r}

# Grab the table of David Bowie recordings from Wikipedia
library(rvest)
library(tidyverse)
library(methods)
url <- "https://en.wikipedia.org/wiki/List_of_songs_recorded_by_David_Bowie"
tables <- url %>%
  read_html() %>%
  html_nodes(css = "table")
tables
songs <- html_table(tables[[3]])
glimpse(songs)


```

First let's clean this up a bit.

```{r}

# Clean the data

songs <- songs %>%
  mutate(Song = gsub('\\"', "", Song), Year = as.numeric(Year)) %>%
  rename(songwriters = `Writer(s)`)

```

It appears that Bowie recorded 418 songs. Who wrote all of them?

```{r}

# Count the number of different songwriters
pattern <- "Bowie"
bowie_wrote <- grepl(pattern, songs$songwriters)
sum(bowie_wrote)


```

Another important concept in text mining is to calculate the _term frequency - inverse document frequency (tf-idf)_, also called the _document term matrix_. The frequency of term _t_ in document _d_ is denoted as _t f (t,d)_ and is equal to the number of times that the term _t_ appears in document _d_. The _inverse document frequency_ measures the prevalence of a term across a set of documents,

\begin{equation}
idf(t,D)=log{\frac{|D|}{|\{d \in D: t \in d\}}}
\end{equation}

Finally, we also use _tf.idf(t,d,D)_ which we calculate as 

\begin{equation}
tf(t,D) * idf(t,D)
\end{equation}

This is frequently use in search engines, when the relevance of a particular word is needed across a corpus.

Commonly used words like "the" and "a" will appear in every document. Thus, their inverse document frequencies will be zero, and their _tf.idf_ will be zero regardless of the term frequency. Documents with high _tf.idf_ scores for a particular term will contain that term many times relative to its appearance across many documents, lending increased relevance of that document to the search term of concern.

The `DocumentTermMatrix()` function in the `tm` (text mining) package will create a document term matrix with one row per document and one column per term. Let's find the term frequency information in the titles of the songs that Bowie sang.

```{r}

library(tm)
song_titles <- VCorpus(VectorSource(songs$Song)) %>%
  tm_map(removeWords, stopwords("english")) %>%
  DocumentTermMatrix(control = list(weighting = weightTfIdf))
findFreqTerms(song_titles, 25)


```
We see that there are a lot of cover songs recorded. Let's fix that then look at the term frequency information again. Looking through the titles, we see two more problems to fix, so let's do those as well.

```{r}

# A bit of clean up
pattern <- "\\(Tin Machine\\)"
songs$Song <- sub(pattern, "", songs$Song)
pattern2 <- "\\(Live\\)"
songs$Song <- sub(pattern2, "", songs$Song)
pattern5 <- "Bowie\\)"
songs$Song <- sub(pattern5, "", songs$Song)
pattern6 <- "!"
songs$Song <- sub(pattern6, "", songs$Song)
pattern4 <- "Segue"
songs$Song <- sub(pattern4, "", songs$Song)
pattern7 <- "\\(Segue\\)"
songs$Song <- sub(pattern7, "", songs$Song)
pattern8 <- "the"
songs$Song <- sub(pattern8, "", songs$Song)
pattern9 <- "The"
songs$Song <- sub(pattern9, "", songs$Song)
pattern10 <- "David"
songs$Song <- sub(pattern10, "", songs$Song)
pattern11 <- "and"
songs$Song <- sub(pattern11, "", songs$Song)

pattern3 <- "\\(\\)"
songs$Song <- sub(pattern3, "", songs$Song)

pattern12 <- "cover)"
songs$Song <- sub(pattern12, "", songs$Song)

#songs$Song[353] <- "Tin Machine"

# Run the frequencies again
song_titles <- VCorpus(VectorSource(songs$Song)) %>%
  tm_map(removeWords, stopwords("english")) %>%
  DocumentTermMatrix(control = list(weighting = weightTfIdf))
#findFreqTerms(song_titles, 25)



```

Another way to look at text is to create a _word cloud_, which you can think of as a multivariate histogram for words. You will be surprised, I'm sure, to learn that R has a `wordcloud` package that will allow you to create this object.

```{r}

# Create wordcloud from Bowie song titles

library(wordcloud)
library(RColorBrewer)
wordcloud(VCorpus(VectorSource(songs$Song)), max.words = 30, scale = c(4, 1), colors = topo.colors(n=30), random.color = TRUE)

```

