---
title: "Text Wrangling"
author: "Vicki Hertzberg"
date: "2/19/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Working with Text

Machines ==> Good at storing text, not so good at understanding text

Humans ==> Good at understanding text, not so good at storing text

To process text, you need an extra set of wrangling skills. We are now going to introduce how to 
- _ingest_ text
- create _corpora_ (collections of text document)
- use _regular expressions_ to automate text searches 

First let's call up the packages we will need:

```{r}

library(RCurl); packageVersion("RCurl")
library(tidyverse); packageVersion("tidyverse")
library(stringr); packageVersion("stringr")
library(rvest); packageVersion("rvest")
library(methods); packageVersion("methods")
library(tm); packageVersion("tm")
library(wordcloud); packageVersion("wordcloud")
library(RColorBrewer); packageVersion("RColorBrewer")


```


We are going to use text mining techniques to explore the play _The Tragedy of Hamlet, Prince of Denmark_, aka _Hamlet_. To do this, we are going to pull the text from the Project Gutenberg site, (http://www.gutenberg.org)[http://www.gutenberg.org]. Project Gutenberg has over 53,000 electronic books and other text documents that you can download _for free_.

```{r}

# Get the text of Hamlet

# We have called the package RCurl previously to allow us to call the URL
hamlet_url <- "http://www.gutenberg.org/cache/epub/1787/pg1787.txt"
Hamlet_raw <- RCurl::getURL(hamlet_url)

```

Note that the object `Hamlet_raw` is a *single string* of text containing the entire play. 

To work with this, we are going to have to split this string into a vector of strings, and we will do this using the function `strsplit()`. We will also have to specify the end of line character(s), which in this case are: `\r\n`.

```{r}

# split string at end of line codes (\r\n); this returns a list - we only want the first element in the list
hamlet <- strsplit(Hamlet_raw, "\r\n")[[1]]
length(hamlet)

```
Now let's examine the text:

```{r}
#examine the text - list out some elements
hamlet[350:360]
```

Notice that in this new `hamlet` object there are two spaces between the start of a line, then an abbreviation for the speaker's name, then another space. We can take advantage of this and other patterns to quantify the ideas within the text. As an example, let's see how many times Hamlet speaks.

```{r}

# get the number of lines that Hamlet speaks
hamlet_lines <- grep("  Ham. ", hamlet, value = TRUE)
length(hamlet_lines)

# inspect the data
head(hamlet_lines)
```

Look at the difference if we don't consider the space after the abbreviation in our expression to evaluate:

```{r}

# get the number of lines that Hamlet speaks
hamlet_lines <- grep("  Ham.", hamlet, value = TRUE)
length(hamlet_lines)

# inspect the data
head(hamlet_lines)
```

We will see that happens down below. 

The `grep()` function is what you use when you want to use R to find a needle in a haystack. The first argument to the function is a _regular expression_ (i.e., a pattern) that you want to find, and the second argument is the character vector in which you want to find the patterns. Note that if you do not include "value = TRUE", the function will return the indices of the haystack in which the needles were found. To illustrate, let's look for Ophelia's lines;

```{r}

# find Ophelia's lines
ophelia_lines <- grep(" Oph. ", hamlet)
length(ophelia_lines)
head(ophelia_lines)
```

The function `grepl()` uses the same syntax but returns instead a logical vector as long as the haystack. For example,

```{r}

#illustrate differences between grep and grepl
length(grep("  Ham. ", hamlet))
length(grepl("  Ham. ", hamlet))
```

To extract the piece of each matching line that actually matched, use the `str_extract()` function from the `stringr` package.

```{r}

# Extract the lines that match
# To do this you need to have called up the package stringr
pattern <- "  Ham. "
grep(pattern, hamlet, value = TRUE) %>%
  str_extract(pattern) %>%
  head()

```

*Regular expressions* are very powerful and very commonly used in many different environments. Understanding the concept of regular expressions will pay off in the long term.

# Regular Expression Syntax

- *.* is a metacharacter that matches any character. If you want to search for the literal values of a metacharacter, you have to use two backslashes in R.

```{r}
# Example of use of . as a metacharacter

hamlet_lines <- grep("  Ham.", hamlet, value = TRUE)
length(hamlet_lines)

hamlet_lines <- grep("  Ham. ", hamlet, value = TRUE)
length(hamlet_lines)

hamlet_lines <- grep("  Ham\\.", hamlet, value = TRUE)
length(hamlet_lines)

```

- *Character sets:* Use brackets to define sets of characters to match.

```{r}

# Example os use of character sets

head(grep("H[b-z]", hamlet, value = TRUE))
```

This notation will result in each occurrence of H followed by any small letter except "a".

- *Alternation:* For this we use the symbol `|` within parentheses.

```{r}

# Example of alternation

head(grep("  H(a|o)", hamlet, value = TRUE))

```

So you see that using the `  H(a|o)` alternation allows us to pick up any lines in which occurs the sequence "  Ha" or "  Ho".

- *Anchors:* Use `^` to anchor a pattern to the beginning of a text, and use `$` to anchor it to the end.

```{r}

# Example of anchor at the beginning
head(grep("^  H[b-z]", hamlet, value = TRUE))

```

- *Repetitions:* Specify the number of times that we want a certain pattern to occur.

    - `?` means zero or one time
    - `*` means zero or more times
    - `+` means one or more times
    
```{r}

# Examples of repetitive patterns

head(grep("  $H(a|o)", hamlet, value = TRUE))

```

```{r}

# Examples of repetitive patterns

head(grep("  *H(a|o)", hamlet, value = TRUE))
```

```{r}

# Examples of repetitive patterns

head(grep("  +H(a|o)", hamlet, value = TRUE))
```

So how do we use these techniques to analyze the text?

- What can we learn by noting who speaks when?
_ When does each character speak as a function of the line number in the play?


```{r}

# Assign the characters
Hamlet <- grepl("  Ham\\.", hamlet)
Ophelia <- grepl("  Oph\\.", hamlet)
Polonius <- grepl("  Pol\\.", hamlet)
Gertrude <- grepl("  Queen\\.", hamlet)
Laertes <- grepl("  Laer\\.", hamlet)
Claudius <- grepl("  King\\.", hamlet)
Horatio <- grepl("  Hor\\.", hamlet)
Fortinbras <- grepl("  For\\.", hamlet)

length(Hamlet)
length(Ophelia)
length(Polonius)
length(Gertrude)
length(Laertes)
length(Claudius)
length(Horatio)
length(Fortinbras)

sum(Hamlet)
sum(Ophelia)
sum(Polonius)
sum(Gertrude)
sum(Laertes)
sum(Claudius)
sum(Horatio)
sum(Fortinbras)


```

Before we can use these data, we want to convert the *logical* vectors into *numeric* vectors, then tidy the data. There is also a bunch of unwanted text at the beginning and end of the raw text, and we want to get rid of it so that our analyses just pertain to the corpus of the play itself, which starts at line 274 and extends to line 5130.

```{r}

# Let's tidy up the dataset
# Naturally we need to have called up the tidyverse package for this to work

speaker_freq <- data.frame(Hamlet, Polonius, Claudius, Horatio, Ophelia, Gertrude, Laertes) %>%
  mutate(line=1:length(hamlet)) %>%
  gather(key = "character", value = "speak", -line) %>%
  mutate(speak = as.numeric(speak)) %>%
  filter(line > 273 & line < 5131)

glimpse(speaker_freq)
  
```

Another thing to do before we do the deep dive: Let's create some context of helpful information, namely the lines at which each Act starts and stops.

```{r}

# Delineate the Acts
acts_idx <- grep("^A[C|c][T|t] [I|V]+", hamlet)
acts_labels <- str_extract(hamlet[acts_idx], "^A[C|c][T|t] [I|V]+")
acts <- data.frame(line=acts_idx, labels = acts_labels)

```

Now let's see when these 4 characters are speaking throughout the play:

```{r}

# Plot the lines at which each character speaks
ggplot(data = speaker_freq, aes(x=line, y=speak)) +
  geom_smooth(aes(color=character), method = "loess", se = 0, span = 0.4) +
  geom_vline(xintercept = acts_idx, color = "darkgray", lty = 3) +
  geom_text(data = acts, aes(y=0.085, label = labels), hjust = "left", color = "darkgray") +
  ylim(c(0, NA)) +
  xlab("Line Number") +
  ylab("Proportion of Speeches")

```

We can also ingest text by scraping it from the internet. Let's look at the discography of David Bowie, as listed on Wikipedia:

** THIS CODE NOT WORKING - FIXED in 2020**

```{r}

# Grab the table of David Bowie recordings from Wikipedia
# We need to have called up the packages rvest and methods to do this.
url <- "https://en.wikipedia.org/wiki/List_of_songs_recorded_by_David_Bowie#Z"
tables <- url %>%
  read_html() %>%
  html_nodes(xpath='//*[@id="mw-content-text"]/div/table[2]')
tables
songs <- html_table(tables[[1]])
glimpse(songs)


```

First let's clean this up a bit.

```{r}

# Clean the data

songs <- songs %>%
  mutate(Song = gsub('\\"', "", Song), Year = as.numeric(Year)) %>%
  rename(songwriters = `Writer(s)`)

```

It appears that Bowie recorded 418 songs. Who wrote all of them?

```{r}

# Count the number of different songwriters
pattern <- "Bowie"
bowie_wrote <- grepl(pattern, songs$songwriters)
sum(bowie_wrote)


```

Another important concept in text mining is to calculate the _term frequency - inverse document frequency (tf-idf)_, also called the _document term matrix_. The frequency of term _t_ in document _d_ is denoted as _t f (t,d)_ and is equal to the number of times that the term _t_ appears in document _d_. The _inverse document frequency_ measures the prevalence of a term across a set of documents,

\begin{equation}
idf(t,D)=log{\frac{|D|}{|\{d \in D: t \in d\}}}
\end{equation}

Finally, we also use _tf.idf(t,d,D)_ which we calculate as 

\begin{equation}
tf(t,D)xidf(t,D)
\end{equation}

This is frequently use in search engines, when the relevance of a particular word is needed across a corpus.

Commonly used words like "the" and "a" will appear in every document. Thus, their inverse document frequencies will be zero, and their _tf.idf_ will be zero regardless of the term frequency. Documents with high _tf.idf_ scores for a particular term will contain that term many times relative to its appearance across many documents, lending increased relevance of that document to the search term of concern.

The `DocumentTermMatrix()` function in the `tm` (text mining) package will create a document term matrix with one row per document and one column per term. Let's find the term frequency information in the titles of the songs that Bowie sang.

```{r}

#We called up the package tm (standing for text mining) to do this.
song_titles <- VCorpus(VectorSource(songs$Song)) %>%
  tm_map(removeWords, stopwords("english")) %>%
  DocumentTermMatrix(control = list(weighting = weightTfIdf))
findFreqTerms(song_titles, 25)


```
We see that songs from the album _Tin Machine_ have "(Tin Machine)" appended to their name. Let's fix that then look at the term frequency information again. Looking through the titles, we see two more problems to fix, so let's do those as well.

```{r}

# A bit of clean up
pattern <- "\\(Tin Machine\\)"
songs$Song <- sub(pattern, "", songs$Song)
pattern2 <- "\\(Live\\)"
songs$Song <- sub(pattern2, "", songs$Song)
pattern5 <- "Bowie\\)"
songs$Song <- sub(pattern5, "", songs$Song)
pattern6 <- "!"
songs$Song <- sub(pattern6, "", songs$Song)
pattern4 <- "Segue"
songs$Song <- sub(pattern4, "", songs$Song)
pattern7 <- "\\(Segue\\)"
songs$Song <- sub(pattern7, "", songs$Song)
pattern8 <- "the"
songs$Song <- sub(pattern8, "", songs$Song)
pattern9 <- "The"
songs$Song <- sub(pattern9, "", songs$Song)
pattern10 <- "David"
songs$Song <- sub(pattern10, "", songs$Song)
pattern11 <- "and"
songs$Song <- sub(pattern11, "", songs$Song)

pattern3 <- "\\(\\)"
songs$Song <- sub(pattern3, "", songs$Song)

songs$Song[353] <- "Tin Machine"

# Run the frequencies again
song_titles <- VCorpus(VectorSource(songs$Song)) %>%
  tm_map(removeWords, stopwords("english")) %>%
  DocumentTermMatrix(control = list(weighting = weightTfIdf))
findFreqTerms(song_titles, 25)



```

Another way to look at text is to create a _word cloud_, which you can think of as a multivariate histogram for words. You will be surprised, I'm sure, to learn that R has a `wordcloud` package that will allow you to create this object.

```{r}

# Create wordcloud from Bowie song titles

#We need to have the package wordcloud and RColorBrewer for this to work.
wordcloud(VCorpus(VectorSource(songs$Song)), max.words = 30, scale = c(4, 1), colors = topo.colors(n=30), random.color = TRUE)

```

